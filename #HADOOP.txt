#HADOOP
1. https://phoenixnap.com/kb/install-hadoop-ubuntu
2. nano Mapper.py
3.

#!/usr/bin/env python3
import sys

for line in sys.stdin:
    for word in line.strip().split():
        print(f"{word}\t1")

4. nano Reducer.py
5.

#!/usr/bin/env python3
import sys

current_word = None
count = 0

for line in sys.stdin:
    word, val = line.strip().split("\t")
    val = int(val)

    if current_word == word:
        count += val
    else:
        if current_word:
            print(f"{current_word}\t{count}")
        current_word = word
        count = val

if current_word:
    print(f"{current_word}\t{count}")

6. chmod +x mapper.py reducer.py
7. hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar \
    -input /input \
    -output /output \
    -mapper mapper.py \
    -reducer reducer.py

#SPARK
1. sudo apt install python3 python3-pip -y
2. python3 --version
3. pip3 --version
4. wget https://downloads.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz
5. tar -xvzf spark-3.5.1-bin-hadoop3.tgz
6. sudo mv spark-3.5.1-bin-hadoop3 /usr/local/spark
7. nano ~/.bashrc

# Java
export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

# Spark
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

# (Optional) Hadoop integration
export HADOOP_HOME=/usr/local/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH

8. source ~/.bashrc
9. nano wordcount.py
10.
from pyspark import SparkContext, SparkConf

def main():
    # Initialize Spark
    conf = SparkConf().setAppName("WordCount").setMaster("local[*]")
    sc = SparkContext(conf=conf)

    # Input file path (can be local or HDFS)
    input_path = "input.txt"
    output_path = "output_wordcount"

    # Read the input file
    text_file = sc.textFile(input_path)

    # Split lines into words
    words = text_file.flatMap(lambda line: line.split())

    # Map each word to (word, 1)
    word_pairs = words.map(lambda word: (word, 1))

    # Reduce by key (sum up the counts)
    word_counts = word_pairs.reduceByKey(lambda a, b: a + b)

    # Save the results
    word_counts.saveAsTextFile(output_path)

    # Optional: Print results to console
    for word, count in word_counts.collect():
        print(f"{word}: {count}")

    sc.stop()

if _name_ == "_main_":
    main()

11. echo "hello world hello spark" > input.txt
12. spark-submit wordcount.py